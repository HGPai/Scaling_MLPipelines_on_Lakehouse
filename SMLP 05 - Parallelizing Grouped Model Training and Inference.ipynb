{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "859a01e8-2be4-4385-bc8e-0abf4273c7d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e9ae22-cfb9-4ced-9e96-90ca82e316de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Parallelizing Grouped Model Training and Inference\n",
    "\n",
    "In this notebook, we'll demonstrate how to apply grouped machine learning model training and inference using Pandas UDFs and the Pandas Function APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b46e6d4c-c941-447c-8b72-1e00c797e954",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the classroom-setup notebook to initialize all of our variables and load our course data.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> You will need to run this in every notebook of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a185da9-f2e3-4fa8-979c-fe4d7224382b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b13691-b043-4063-b20c-ff4e4471f943",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference with Pandas UDFs\n",
    "\n",
    "In this part of the noteboook, we'll demonstrate how to parallelize single-node model inference using vectorized [Pandas UDFs](https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html).\n",
    "\n",
    "#### Split data\n",
    "\n",
    "First, we're going to load two different sets:\n",
    "\n",
    "1. A **modeling set** to facilitate modeling &mdash; this will then get split into a training and a test set\n",
    "2. An **inference set** to facilitate the demonstration of parallelizing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b98274af-5473-4147-a8ef-a3400cba14be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_df = spark.read.format(\"delta\").load(lesson_5_model_path)\n",
    "model_pdf = model_df.toPandas()\n",
    "\n",
    "inference_df = spark.read.format(\"delta\").load(lesson_5_inference_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd98f0c2-6166-4b1e-81f5-4324e510fffc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Build and log model\n",
    "\n",
    "In order to demonstrate this process, we'll need to build and log a simple single-node model.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> It's important that we log this model so we can easily load it back in during inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90f0c515-88ac-42b8-a4d6-74650dd2a4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mlflow.set_experiment(\"/Users/\" + username + \"/SMLP-Lesson-5-SN\")\n",
    "with mlflow.start_run(run_name=\"sklearn-random-forest\") as run:\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        model_pdf.drop([\"price\", \"neighbourhood_cleansed\", \"id\"], axis=1), \n",
    "        model_pdf[[\"price\"]].values.ravel(),\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    rfr = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rfr.fit(X_train, y_train)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rfr, \"random-forest-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e1cbbf9-4e30-4e6b-91a6-02ef257b2ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Pandas UDF for prediction\n",
    "\n",
    "Next, we need to define the Pandas UDF we'll use for prediction.\n",
    "\n",
    "Our first step is to decorate the function using the `@pandas_udf` decorator.\n",
    "\n",
    "Next, we create a function with the following signature:\n",
    "\n",
    "* **Input**: An iterator of DataFrames\n",
    "* **Output**: An iterator of series\n",
    "\n",
    "This will look similar to the iterator of series -> iterator of series [workflow used here](https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html#iterator-of-series-to-iterator-of-series-udf).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> When we use an iterator as input/output, we only need to load the model once per executor rather than with each batch/partition &mdash; this reduces overhead to help us scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22d61650-aaae-4a53-a364-9e929717d0ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Use the pandas_udf decorator and indicate that a double-type value is being returned\n",
    "@pandas_udf(\"double\")\n",
    "def udf_predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n",
    "    \n",
    "    # Load the model for the entire iterator\n",
    "    model_path = f\"runs:/{run.info.run_id}/random-forest-model\" \n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "    \n",
    "    # Iterate through and get a prediction for each batch (partition)\n",
    "    for features in iterator:\n",
    "        pdf = pd.concat(features, axis=1)\n",
    "        yield pd.Series(model.predict(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48bc7cd5-9f52-4672-a0b5-824b93d7030e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apply Pandas UDF to Spark DataFrame\n",
    "\n",
    "And finally, we can apply our single-node model to our Spark DataFrame in parallel.\n",
    "\n",
    "Even though the API is relatively simple, recall everything happening here:\n",
    "\n",
    "1. Each partition of `inference_df` is being converted to a Pandas DataFrame using Apache Arrow\n",
    "2. The model is being loaded on each executor\n",
    "3. An iterator of Pandas DataFrames is passed to each executor\n",
    "4. A Pandas Series is returned from each executor and converted back to Spark using Apace Arrow\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Because we need to broadcast the model onto each executor, this approach could become inefficient if your model is extremely large in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "299938df-e8b7-4a3c-9d41-501dece9b1de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_df = inference_df.withColumn(\n",
    "    \"prediction\", \n",
    "    udf_predict(*inference_df.drop(\"price\", \"neighbourhood_cleansed\", \"id\").columns)\n",
    ")\n",
    "display(prediction_df.select(\"price\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7394bf77-02a4-4ed7-9322-916ad6b594b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In addition to using the Pandas UDF `predict` to apply the model to a Spark DataFrame, we can also use MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3439a0f3-79b4-4661-8277-05024fd40aac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow_predict = mlflow.pyfunc.spark_udf(spark, run.info.artifact_uri + \"/random-forest-model\")\n",
    "prediction_df = inference_df.withColumn(\n",
    "    \"prediction\", \n",
    "    mlflow_predict(*inference_df.drop(\"price\", \"neighbourhood_cleansed\", \"id\").columns)\n",
    ")\n",
    "display(prediction_df.select(\"price\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b8bea9-8413-4233-9c15-73c45254196d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parallelizing Grouped Training with the Pandas Function API\n",
    "\n",
    "In this part of the notebook, we'll demo how to parallelize the training of group-specific single-node models with [the Pandas Function API](https://docs.databricks.com/spark/latest/spark-sql/pandas-function-apis.html).\n",
    "\n",
    "#### Define `train_model` function\n",
    "\n",
    "First, we need to create our `train_model` function using nested MLflow runs.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> We are returning metadata from this function. This will be helpful when we perform grouped model inference later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c2324e-f9ac-4554-b24c-b97dad84cb8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Pull metadata\n",
    "    neighbourhood = df_pandas[\"neighbourhood_cleansed\"].iloc[0] # This works because df_pandas is neighborhood-specific\n",
    "    n_used = df_pandas.shape[0]\n",
    "    run_id = df_pandas[\"run_id\"].iloc[0]                 # Pulls run ID to do a nested run\n",
    "    experiment_id = df_pandas[\"experiment_id\"].iloc[0]   # Pulls experiment ID for Jobs\n",
    "\n",
    "    # Train the model\n",
    "    X = df_pandas.drop([\"price\", \"neighbourhood_cleansed\", \"run_id\", \"experiment_id\", \"id\"], axis=1)\n",
    "    y = df_pandas[\"price\"]\n",
    "    rfr = RandomForestRegressor(n_estimators=10, max_depth=8)\n",
    "    rfr.fit(X, y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = rfr.predict(X)\n",
    "    rmse = mean_squared_error(y, predictions, squared=False)\n",
    "\n",
    "    # Resume the top-level training\n",
    "    with mlflow.start_run(run_id=run_id, experiment_id=experiment_id):\n",
    "        \n",
    "        # Create a nested run for the specific nieghbourhood\n",
    "        with mlflow.start_run(run_name=neighbourhood, experiment_id=experiment_id, nested=True) as run:\n",
    "            mlflow.sklearn.log_model(rfr, neighbourhood)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "            artifact_uri = f\"runs:/{run.info.run_id}/{neighbourhood}\"\n",
    "            \n",
    "            # Create a return pandas DataFrame that matches the schema above\n",
    "            return_df = pd.DataFrame(\n",
    "                [[neighbourhood, n_used, artifact_uri, rmse]], \n",
    "                columns=[\"neighbourhood_cleansed\", \"n_used\", \"model_path\", \"rmse\"]\n",
    "            )\n",
    "\n",
    "    return return_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73de5347-8932-4a54-b884-b91fa742d6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define return schema\n",
    "\n",
    "We need to define the schema of the DataFrame being returned from our `train_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a486c83-69c5-41f8-a27d-c87975571c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructField, StructType\n",
    "\n",
    "return_schema = StructType([\n",
    "    StructField(\"neighbourhood_cleansed\", StringType()),  # unique nieghbourhood name\n",
    "    StructField(\"n_used\", IntegerType()),                 # number of records used in training\n",
    "    StructField(\"model_path\", StringType()),              # path to the model for a given neighbourhood\n",
    "    StructField(\"rmse\", FloatType())                      # metric for model performance\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f83c42c-788c-4f65-99c2-8c24a2059279",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apply `train_model` function to each group\n",
    "\n",
    "Now we apply the `train_model` function to each group.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Notice how we are passing the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6567341-e46c-45bc-b43f-4ed819eb4941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Start parent run\n",
    "mlflow.set_experiment(\"/Users/\" + username + \"/SMLP-Lesson-5-Grouped\")\n",
    "with mlflow.start_run(run_name=\"Training session for all neighborhood\") as run:\n",
    "    \n",
    "    # Get run_id and experiment_id\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = mlflow.get_experiment_by_name(\"/Users/\" + username + \"/SMLP-Lesson-5-Grouped\").experiment_id\n",
    "\n",
    "    # Apply function to each group\n",
    "    train_output_df = (model_df\n",
    "        .withColumn(\"run_id\", lit(run_id))                     # Add run_id to pass into function\n",
    "        .withColumn(\"experiment_id\", lit(experiment_id))       # Add experiment_id to pass into function\n",
    "        .groupby(\"neighbourhood_cleansed\")                     # Group by neighbourhood\n",
    "        .applyInPandas(train_model, schema=return_schema)      # Apply train_model function for each neighbourhood\n",
    "    )\n",
    "\n",
    "display(train_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4a9919-551e-4186-b417-c794ef0aad1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parallelizing Grouped Inference with the Pandas Function API\n",
    "\n",
    "In this part of the notebook, we'll demo how to parallelize the training of group-specific single-node models.\n",
    "\n",
    "#### Combine inference data and metadata\n",
    "\n",
    "We need to start by combining the `inference_df` and the `train_output_df` into a single `combined_df`. This provides an easy way to pass all of the information into the `apply_model` function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdef3468-634b-44a0-8a8b-bb9aeb70b3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = inference_df.join(train_output_df, \"neighbourhood_cleansed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3a0628-f775-45a7-8a8c-47ecfc91bc05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define `apply_model` function\n",
    "\n",
    "From here on, this process looks very similar to how group-specific training was parallelized above:\n",
    "\n",
    "1. Define an `apply_model` function to apply\n",
    "2. Define a `return_schema` for the return DataFrame of `apply_model`\n",
    "3. Split the data by group, apply the function, and return the combined results\n",
    "\n",
    "We'll start by defining the `apply_model` function.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Be sure to drop the metadata columns we passed in with the DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e5e0ef-c509-47b5-a1ff-f9516239d464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Get model path from metadata\n",
    "    model_path = df_pandas[\"model_path\"].iloc[0]\n",
    "\n",
    "    # Subset inference set to features\n",
    "    X = df_pandas.drop([\"price\", \"neighbourhood_cleansed\", \"id\", \"n_used\", \"rmse\", \"model_path\"], axis=1)\n",
    "\n",
    "    # Load and apply model to inference set\n",
    "    model = mlflow.sklearn.load_model(model_path)\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    # Create return DataFrame\n",
    "    return_df = pd.DataFrame({\n",
    "        \"id\": df_pandas[\"id\"],      # A unique identifier is key to linking predictions back to the DF later\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d204044-5fc9-40ae-963f-65d8df321532",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define return schema\n",
    "\n",
    "And again, we define a return schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759d5565-634a-44f7-9a69-3d09a9b122be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "return_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"prediction\", FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a58139-8c1d-41e5-b262-b5e24c51696f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apply `apply_model` function to each group\n",
    "\n",
    "And now we follow the split-apply-combine workflow using the `apply_model` function.\n",
    "\n",
    "The resulting predictions were each computed using each group's respective model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46310a68-6e74-43a5-8b52-72bdc053b3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_df = combined_df.groupby(\"neighbourhood_cleansed\").applyInPandas(apply_model, schema=return_schema)\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab9606e4-44a1-4c66-ac31-07083c2feda6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2021 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SMLP 05 - Parallelizing Grouped Model Training and Inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
