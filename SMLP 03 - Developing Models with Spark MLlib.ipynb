{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acaa98c-3df0-4c7c-805e-30ec8b2590a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e176a9f-70ec-444d-a618-4276100e8938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Developing Models with Spark MLlib\n",
    "\n",
    "In this notebook, we'll demonstrate how scale machine learning model development using Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e63f7a66-4d65-474b-826e-e043540df1fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the classroom-setup notebook to initialize all of our variables and load our course data.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> You will need to run this in every notebook of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46554a67-b3d8-484d-8609-1b8828b5dcd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd7fbf22-b514-480b-9c03-f45e5af5c7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Spark MLlib Model Workflow\n",
    "\n",
    "In this part of the demo, we will demonstrate a basic workflow to develop models using the Spark MLlib API.\n",
    "\n",
    "#### Load prepared data\n",
    "\n",
    "First, we are going to load the data we prepared in the last lab. Remember that this is one-hot encoded, imputed, and has a features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba98a2d-c5e2-4473-b6a9-61c780dd00a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_vector_train_df = spark.read.format(\"delta\").load(lesson_3_train_feature_vector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60822bd0-ad33-4ab8-8ba7-7f4b47eee4f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model Training\n",
    "\n",
    "Next, we're going to train our model. Each modeling algorithm in Spark MLlib is an **estimator**, so we need to first instantiate it with the necessary arguments and then call its `fit` method on the training DataFrame.\n",
    "\n",
    "In this case, we're going to use the [**`LinearRegression`** class](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression) to predict the `price` target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee9a9604-9869-4dea-bfc7-7b79eaab3989",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(feature_vector_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3800d260-a8a8-4ebf-b864-d729b94d40a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model Inference\n",
    "\n",
    "In order to evaluate our model's quality, we need to first get the predictions for our training DataFrame.\n",
    "\n",
    "We can use the [fitted **`LinearRegressionModel`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html#pyspark.ml.regression.LinearRegressionModel) `lr_model` to complete this â€“ it's a special kind of **transformer** called a **model**, so we can use its `transform` method on the training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20bad33a-4b97-4e2b-9be2-1f84340a763c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_train_df = lr_model.transform(feature_vector_train_df).select(\"price\", \"prediction\")\n",
    "display(predictions_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bcc876f-3f3d-45b0-bd9b-5dc1225076c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Now that we have our predictions for our training DataFrame, we need to evaluate the quality of our model.\n",
    "\n",
    "We can do this using an **evaluator** &mdash; another type of Spark MLlib.\n",
    "\n",
    "There are a number of [types of evaluators in Spark MLlib](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#evaluation), but three that are commonly used are:\n",
    "\n",
    "1. [The `RegressionEvaluator`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html#pyspark.ml.evaluation.RegressionEvaluator)\n",
    "2. [The `BinaryClassificationEvaluator`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html#pyspark.ml.evaluation.RegressionEvaluator)\n",
    "3. [The `MulticlassClassificationEvaluator`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)\n",
    "\n",
    "Since we are working on a regression problem here, we'll use the `RegressionEvaluator` class. We can set a specific evaluation metric and use the `evaluate` method compute that evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ec138c-cff5-4b66-ad62-bfbb16bfa115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_train_df)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions_train_df)\n",
    "print(f\"The training RMSE = {rmse}\")\n",
    "print(f\"The training R^2 = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bd136fc-53cb-41f8-a38a-cbaaae13e0e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You might be wondering why we aren't working with the test DataFrame at this point &mdash; we'll cover it now with the `Pipeline` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee087a6a-0273-4b5e-b995-9a9dfaa9e6b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark MLlib Pipeline API\n",
    "\n",
    "In this part of the demo, we'll demonstrate how to use the [Spark MLlib Pipeline API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html#pyspark.ml.Pipeline). We can use Pipelines for data preparation stages only, or we can create full-on modeling pipelines.\n",
    "\n",
    "To utilize the Pipeline API, we must first instantiate the necessary estimator/transformer objects. \n",
    "\n",
    "We have three key classes we're going to use:\n",
    "\n",
    "1. `Imputer` &mdash; Imputing missing values\n",
    "2. `VectorAssembler` &mdash; Assembling a feature vector\n",
    "3. `LinearRegression` &mdash; Training a linear regression model\n",
    "\n",
    "First, we'll load in our unprepared training and test sets &mdash; note that these versions only contain numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b2a77f-135f-415d-a635-3955fa4fc3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.format(\"delta\").load(lesson_3_train_path)\n",
    "test_df = spark.read.format(\"delta\").load(lesson_3_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38440a6b-cec9-4516-9817-78837ce6f10b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we'll create the imputer. **Notice that we aren't fitting the imputer here.**\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> We still need to programmatically determine which feature columns we should use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c07af30-8f9d-4cf5-a163-df476fb28123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Identify columns with missing values\n",
    "double_cols = [column.name for column in train_df.schema.fields if column.dataType == DoubleType() and column.name != \"price\"]\n",
    "missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in double_cols]\n",
    "row_dict = train_df.select(missing_values_logic).first().asDict()\n",
    "missing_cols = [column for column in row_dict if row_dict[column] > 0]\n",
    "\n",
    "# Create the imputer\n",
    "imputer = Imputer(strategy=\"median\", inputCols=missing_cols, outputCols=missing_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e958d3-e306-4f0d-9563-dc9bcaff2d05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we'll create the vector assembler. Again, notice that we are not transforming the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27b18e3-9636-489e-ada9-e8ec6e76f80f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create the vector assembler\n",
    "feature_cols = [column.name for column in train_df.schema.fields if column.dataType == DoubleType() and column.name != \"price\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "749e550e-a898-45ac-bad0-91452f024c1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And finally, we create the untrained linear regression estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfb49a48-3315-41c0-bf5d-9ae7cc1ca3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create linear regression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b3b671-af69-4ce3-8e43-1a6e834700dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once we have each of our untrained estimators and transformers created, we can put them into stages as a Spark MLlib Pipeline.\n",
    "\n",
    "We can then call the [Pipeline's **`fit`** method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html#pyspark.ml.Pipeline.fit) on the training data to fit each of the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbf606c4-ddfd-4aff-9e71-effac2f421ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[imputer, vector_assembler, lr])\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1001f7e0-16f1-4cab-a7b9-7dc4a24bdca6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And then we can use the fit [PipelineModel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.PipelineModel.html#pyspark.ml.PipelineModel) to transform our initial training DataFrame. \n",
    "\n",
    "This will pass our data through each of the fitted transformers, including the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b6a723-0d15-401e-ba94-cc6378f1382c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_preds_df = pipeline_model.transform(train_df).select(\"price\", \"prediction\")\n",
    "display(train_preds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d64647c5-3fc0-4b5d-b432-9432a42970e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also use the fit PipelineModel to transform the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea3bb13-2d8d-43b5-8a4b-0c896de262d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_preds_df = pipeline_model.transform(test_df).select(\"price\", \"prediction\")\n",
    "display(test_preds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ea957b-75ce-45ed-8c40-5b10b6813ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Distributed Decision Trees\n",
    "\n",
    "Next, we'll investigate the implications of distributed algorithms by creating a decision tree using Spark MLlib.\n",
    "\n",
    "Based on the prerequisites for this course, we're assuming that you are familiar with decision trees. If you need a refreshed, we recommend checking out the [R2D3's explanation](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/).\n",
    "\n",
    "#### Load prepared data\n",
    "\n",
    "We are going to load a version of our London-based listing data that's been prepared for tree-based models.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Remember that tree-based models don't perform well with one-hot encoded features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ff2d64-ecfb-41ea-aba2-d37493b2be55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "tree_train_df = spark.read.format(\"delta\").load(lesson_3_train_tree_path)\n",
    "\n",
    "# Prepare Spark data\n",
    "feature_cols = [column for column in tree_train_df.columns if column != \"price\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "spark_tree_train_df = vector_assembler.transform(tree_train_df)\n",
    "\n",
    "# Prepare Sklearn data\n",
    "pandas_tree_train_df = tree_train_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30586ab0-6075-40e8-a0be-e27d44315802",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark MLlib vs. Scikit-learn\n",
    "\n",
    "As we mentioned, sometimes learning algorithms differ when they're distributed vs. single-node. In order to demonstrate this, we're going to train a decision tree in Spark MLlib and Scikit-learn and compare their predictions.\n",
    "\n",
    "We'll start with [Spark MLlib's **`DecisionTreeRegressor`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Recognize that Spark MLlib's API workflow is similar across model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500ca6fa-e963-4fa8-bac5-02cc8d52d5d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\", maxDepth=10)\n",
    "dtr_model = dtr.fit(spark_tree_train_df)\n",
    "\n",
    "train_preds_sparkmllib = dtr_model.transform(spark_tree_train_df).select(\"price\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ca9750-5de8-4db5-bbdb-90254515ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we'll build a single-node decision tree with [Scikit-learn's **`DecisionTreeRegressor`**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> For comparison, it's important to note that these runs have the same tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e0303c6-d904-4777-8cba-5b57fc5eaf32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = pandas_tree_train_df.drop(\"price\", axis=1)\n",
    "y = pandas_tree_train_df[\"price\"]\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth=10)\n",
    "dtr_model = dtr.fit(X, y)\n",
    "\n",
    "train_preds_sklearn = dtr_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72169804-2fce-4d19-9d8c-2dedaa771255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And once we have both models, we can compute the correlation between the predictions for each model.\n",
    "\n",
    "**Question:** What do you think the correlation will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d470b3d-af5a-4ef8-97dc-e878dd9be159",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_preds_df = train_preds_sparkmllib \\\n",
    "    .select(col(\"price\").alias(\"Actual\"), col(\"prediction\").alias(\"Spark MLlib\")) \\\n",
    "    .toPandas()\n",
    "\n",
    "full_preds_df[\"Scikit-learn\"] = train_preds_sklearn\n",
    "\n",
    "display(full_preds_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78371df5-9cf2-4c4f-8389-5cb04854b540",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There's a strong correlation &mdash; but they aren't identical. \n",
    "\n",
    "There could be a few reasons for this, but a big one is that **tweaks have been made to the distributed algorithm**.\n",
    "\n",
    "#### Discretizing Continuous Features\n",
    "\n",
    "A typical decision tree algorithm will consider and assess **all unique values in a continuous feature** for a split. This is what Scikit-learn is doing.\n",
    "\n",
    "![Single-node tree splits](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/single-node-tree-splits.png)\n",
    "\n",
    "But when your data is large and is distributed across multiple worker nodes in a cluster, assessing each unique value for each split can be **prohibitively computationally expensive**. \n",
    "\n",
    "As a result, Spark MLlib [shortcuts this process](https://spark.apache.org/docs/latest/mllib-decision-tree.html#split-candidates) by **discretizing continuous features** into a specified number of bins, denoted by [the **`maxBins`** parameter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor.maxBins). This reduces the number of split candidates considered which makes the training of the decision tree on large, distributed data more efficient.\n",
    "\n",
    "![Distributed tree splits](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/distributed-tree-splits.png)\n",
    "\n",
    "Now, `maxBins` is a tunable parameter just like other hyperparameters:\n",
    "\n",
    "* As `maxBins` increases, the algorithm will consider a larger number of split candidates. It also increases the need for communication among workers and reduces the speed at which the algorithm runs.\n",
    "* As `maxBins` decreases, the algorithm with consider fewer split candidates likely reducing the predictive power of the model. However, the algorithm will run more efficiently.\n",
    "\n",
    "The default value of `maxBins` is 32, but the optimal value depends on your use case and relative need for speed vs. predictive performance.\n",
    "\n",
    "#### Binning Categorical Features\n",
    "\n",
    "The training data we've been using for decision trees so far has only been made up of numeric features, but categorical features can be really helpful!\n",
    "\n",
    "Spark MLlib's decision tree implementation handles categorical features really well &mdash; the recommended best practice is to simply apply the `StringIndexer` to categorical features and the algorithm will recognize that they are categorical rather than ordinally discrete.\n",
    "\n",
    "In the below cell, we load in data that's been prepared using the `StringIndexer` class on categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99a3c1d-32ff-451a-826f-7ee7c67ed1fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tree_cat_train_df = spark.read.format(\"delta\").load(lesson_3_train_tree_cat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3192b17b-5ccf-4a7e-a817-bc9ad6ca5a49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, let's train a Spark MLlib decision tree using that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54ed922-b57f-441e-8881-a343aeb9783f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Uncomment below line to fit the tree\n",
    "# dtr_model = dtr.fit(tree_cat_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb8a6a5e-5320-4add-aded-7e27678a9208",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Oops! We see the below error here: \n",
    "\n",
    "```\n",
    "DecisionTree requires maxBins (= 32) to be at least as large as the number of \n",
    "values in each categorical feature, but categorical feature 49 has 515 values. \n",
    "Consider removing this and other categorical features with a large number of \n",
    "values, or add more training examples.\n",
    "```\n",
    "\n",
    "We received this error because Spark MLlib [uses bins for the indexed categorical features](https://spark.apache.org/docs/latest/mllib-decision-tree.html#split-candidates), as well &mdash; and it doesn't know what to do if there are more categories than available bins for a given feature.\n",
    "\n",
    "![Categorical binning](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/distributed-categorical-tree-bins.png)\n",
    "\n",
    "As a result, we need to make sure that **`maxBins` is at least as large as the greatest cardinality of all of our categorical features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5c8835-8540-4b2e-8db3-e5d67d118f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\", maxBins=515)\n",
    "dtr_model = dtr.fit(feature_vector_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b308d4a6-a590-4dda-b660-87c3fe806268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Great! That fixed our problem.\n",
    "\n",
    "Issues like this are why it's important to understand that distributed algorithms can differ from their single-node counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f6ac1f-c4a9-48ea-8650-965814a4caad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Scaling Hyperparameter Tuning with Spark MLlib\n",
    "\n",
    "In this final part of this lesson's demo, we're going to show how to speed up hyperparameter tuning for Spark MLlib models.\n",
    "\n",
    "#### Decision Tree Pipeline\n",
    "\n",
    "For this demo, we'll be using Spark MLlib's decision tree algorithm.\n",
    "\n",
    "This is our first step in scaling hyperparameter tuning of Spark MLlib models.\n",
    "\n",
    ":NOTE: Because this is a tree-based model, we'll be creating a tree-based data preparation pipeline with a decision tree as its last stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29afc510-acec-4eec-996f-bb8f742392a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Load imputed data\n",
    "train_df = spark.read.format(\"delta\").load(lesson_3_train_path_imp)\n",
    "test_df = spark.read.format(\"delta\").load(lesson_3_test_path_imp)\n",
    "\n",
    "# StringIndexer\n",
    "categorical_cols = [column.name for column in train_df.schema.fields if column.dataType == StringType()]\n",
    "index_cols = [column + \"_index\" for column in categorical_cols]\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "\n",
    "# VectorAssembler\n",
    "numeric_cols = [column.name for column in train_df.schema.fields if column.dataType == DoubleType() and column.name != \"price\"]\n",
    "feature_cols = numeric_cols + index_cols\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\", maxBins=600)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer, vector_assembler, dtr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe42ef5-1705-49df-8901-66e001a6a0ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Parameter Grid\n",
    "\n",
    "Spark MLlib supports **grid search for hyperparameter tuning**.\n",
    "\n",
    "![grid-search](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/grid-search.png)\n",
    "\n",
    "So the first thing we need to do is build a hyperparameter value grid with [Spark MLlib's **`ParamGridBuilder`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html#pyspark.ml.tuning.ParamGridBuilder). This is where we specify the hyperparameter values we'd like to assess.\n",
    "\n",
    "Because we are using [the **`DecisionTreeRegressor`** class](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor), we'll test the following values:\n",
    "\n",
    "* `maxDepth` (max depth of each decision tree): 2, 5\n",
    "* `minInfoGain` (minimum information gain for a split to be considered at a tree node): 0, 0.1\n",
    "\n",
    "The `addGrid` method accepts the name of the parameter (e.g. `dtr.maxDepth`), and a list of the possible values (e.g. `[2, 5]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dde843b-383c-40c6-bc3a-676aff09be60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(dtr.maxDepth, [5, 8])\n",
    "    .addGrid(dtr.minInfoGain, [0, 0.1])\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531776ea-dbc0-454f-923a-f0a01a8804c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cross-validation\n",
    "\n",
    "Since we are tuning hyperparameters, **we need a validation set** to assess model performance within our optimization process &mdash; this keeps us from optimizing on our test or holdout set!\n",
    "\n",
    "To do this, we'll use 3-fold cross-validation with Spark MLlib's `CrossValidator`.\n",
    "\n",
    "![3-fold-CV](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/3-fold-cv.png)\n",
    "\n",
    "With 3-fold cross-validation, we train on 2/3 of the data (the training set), and evaluate with the remaining (validation) 1/3 (the validation set). **We repeat this process 3 times**, so each fold gets the chance to act as the validation set. We then **average the results of the three rounds** to identify which hyperparameter perform the best, on average.\n",
    "\n",
    "When putting together a [Spark MLlib **`CrossValidator`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html#pyspark.ml.tuning.CrossValidator), we pass in the `estimator` (pipeline), `evaluator`, and `estimatorParamMaps` (parameter grid) to `CrossValidator` so that it knows:\n",
    "* Which model to use\n",
    "* How to evaluate the model\n",
    "* What hyperparameter values to assess\n",
    "\n",
    "We can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6fb0408-02fa-48cc-9d28-559284f68187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    evaluator=evaluator, \n",
    "    estimatorParamMaps=param_grid,              \n",
    "    numFolds=3,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bcaa71b-7361-45bd-9885-adef68784f34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The [**`parallelism`** parameter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html#pyspark.ml.tuning.CrossValidator.parallelism) here is defined in the Spark documentation as: \n",
    "\n",
    "> the number of threads to use when running parallel algorithms (>= 1).\n",
    "\n",
    "**Question**: Will increasing the `parallelism` parameter always help speed up the tuning process?\n",
    "\n",
    "It depends, increasing the parallelism will:\n",
    "\n",
    "1. Increase the number of models that can run concurrently\n",
    "2. Decrease the amount of resources available to each model\n",
    "\n",
    "The optimal value for `parallelism` depends on things like the size of your data, your cluster configuration, and the type of modeling algorithm you're using. It's recommended that you **tune this `parallelism` parameter to optimize on speed for your given pipeline**.\n",
    "\n",
    "Next, we can call the `CrossValidator` object's `fit` method to fit the entire pipeline for each unique combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55de3303-8092-421b-9793-8ce1ab8ee1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "104f0f5f-8d4c-4211-b7d1-33858e47d74e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By default, [our **`CrossValidatorModel`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidatorModel.html#pyspark.ml.tuning.CrossValidatorModel) will retrain on the entire `train_df` with the optimal hyperparameters it identified during the cross-validation and grid-search processes.\n",
    "\n",
    "We can access this retrained model for inference using the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e5b32dd-18cc-4fee-b98d-5a610d5b6b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_preds_df = cv_model.transform(train_df)\n",
    "display(train_preds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209504c3-2661-4d05-ac90-c0f164418f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pipeline and CrossValidator Placement\n",
    "\n",
    "In the above demonstration, we placed our `Pipeline` object inside of our `CrossValidator` object as the estimator.\n",
    "\n",
    "There's a major scale-related consequence of this: **the entire data preparation pipeline will be fit and run with each model during the hyperparameter tuning and cross-validation process!** \n",
    "\n",
    "![pipeline-in-crossvalidator](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/pipeline-in-crossvalidator.png)\n",
    "\n",
    "If the data preparation stages of the pipeline are time-consuming, they will dramatically slow down the cross-validation process.\n",
    "\n",
    "An alternative approach is to place the `CrossValidator` object inside of the `Pipeline` object. This way, **we only need to prepare the data preparation pipeline one time**.\n",
    "\n",
    "![crossvalidator-in-pipeline](https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/mlewd/Scaling-Machine-Learning-Pipelines/crossvalidator-in-pipeline.png)\n",
    "\n",
    "Here's a demonstration of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab34f7c2-3e96-472b-b109-34190adc5619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put the model estimator in the cross-validator \n",
    "cv = CrossValidator(\n",
    "    estimator=dtr, \n",
    "    evaluator=evaluator, \n",
    "    estimatorParamMaps=param_grid,              \n",
    "    numFolds=3,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Put the cross validator in the pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer, vector_assembler, cv])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "train_preds_df = pipeline_model.transform(train_df).select(\"features\", \"price\", \"prediction\")\n",
    "display(train_preds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da735263-4d7e-48bb-8c07-9767c0c8d7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Question:** So should we always put the cross-validator within the pipeline?\n",
    "\n",
    "Not necessarily. When we put the cross-validator in the pipeline, we are opening ourselves up to **feature data leakage** in the data preparation stages of our pipeline. With the cross-validator at the end of the pipeline, our early stages will fit on the entirety of the cross-validation data &mdash; this will leak feature information from the validation set to the training set for each model.\n",
    "\n",
    "Whether you choose to put the pipeline in the cross-validator or the cross-validator in the pipeline **depends on your relative needs for scale and avoiding leakage of feature information**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e868bf-8e35-4fe1-9ba9-eefaa75e006d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2021 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SMLP 03 - Developing Models with Spark MLlib",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
