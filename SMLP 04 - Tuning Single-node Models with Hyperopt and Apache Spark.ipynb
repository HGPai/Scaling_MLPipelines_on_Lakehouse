{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7070f1bc-98ba-4df9-8c1b-31286104b42a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d71c83d0-949f-40ec-8f86-c4ae615d6333",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tune Single-node Models with Hyperopt and Apache Spark\n",
    "\n",
    "In this notebook, we'll demonstrate how tune single-node machine learning models using Hyperopt and Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5018db70-6ede-4dc5-bd97-2d9de85dea88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the classroom-setup notebook to initialize all of our variables and load our course data.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> You will need to run this in every notebook of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afedbc57-5ffc-48e9-afb3-437804792774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6710cc31-52ab-40da-8110-f16363f0687d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, we will load our data for this lesson. It's London listing data with only numeric features.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> We'll be working with Scikit-learn in this lesson, so we're going to work with a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9a2b77-7025-47f6-addc-275c1fcbc2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_pdf = spark.read.format(\"delta\").load(lesson_4_train_path).toPandas()\n",
    "test_pdf = spark.read.format(\"delta\").load(lesson_4_test_path).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f720b724-a240-4a34-a8e2-a2a5c844771f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we'll assign common data variables for use with Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a109d2-f99a-4eac-8d59-40969716ed84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_pdf.drop([\"price\"], axis=1)\n",
    "X_test = test_pdf.drop([\"price\"], axis=1)\n",
    "\n",
    "y_train = train_pdf[\"price\"]\n",
    "y_test = test_pdf[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e91eeaa9-df5a-4097-aebd-40eddef8ec05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperopt Workflow\n",
    "\n",
    "Next, we will create the different pieces needed for parallelizing hyperparameter tuning with [Hyperopt](http://hyperopt.github.io/hyperopt/) and Apache Spark.\n",
    "\n",
    "### Create objective function\n",
    "\n",
    "First, we need to [create an **objective function**](http://hyperopt.github.io/hyperopt/getting-started/minimizing_functions/). This is the function that Hyperopt will call for each set of inputs.\n",
    "\n",
    "The basic requirements are:\n",
    "\n",
    "1. An **input** `params` including hyperparameter values to use when training the model\n",
    "2. An **output** containing a loss metric on which to optimize\n",
    "\n",
    "In this case, we are specifying values of `max_depth` and `n_estimators` and returning the RMSE as our loss metric.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Notice that we are cross-validating within this function with `cross_val_score`! Remember that this drastically increases the number of models we need to compute. If you're really crunched for time, use a train-validation split instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c71e3c7-e72b-4aec-ba23-7b8c56abbef2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from numpy import mean\n",
    "from hyperopt import STATUS_OK\n",
    "  \n",
    "def objective_function(params):\n",
    "\n",
    "    # Set the hyperparameters that we want to tune:\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "\n",
    "    regressor = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=42)\n",
    "\n",
    "    # Compute the average cross-validation metric\n",
    "    mse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "    cv_rmse = mean(cross_val_score(regressor, X_train, y_train, scoring=mse_scorer, cv=3))\n",
    "    \n",
    "    return {\"loss\": cv_rmse, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db63722b-666e-44e5-9cb2-5ca852c8554d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define the search space\n",
    "\n",
    "Next, we need to [define the **search space**](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/).\n",
    "\n",
    "To do this, we need to import Hyperopt and use its `quniform` function to specify the range for each hyperparameter.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Remember that we aren't defining the actual values like grid search. Hyperopt's TPE algorithm will intelligently suggest hyperparameter values from within this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e813ea2b-5b3a-4700-bef0-ca207809b7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "  \"max_depth\": hp.quniform(\"max_depth\", 1, 10, 1),\n",
    "  \"n_estimators\": hp.quniform(\"n_estimators\", 5, 50, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba0ed26-46fb-45b1-ba18-948f2995c1a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Call the `fmin` operation\n",
    "\n",
    "The `fmin` function is where we put Hyperopt to work.\n",
    "\n",
    "To make this work, we need:\n",
    "\n",
    "1. The `objective_function`\n",
    "2. The `search_space`\n",
    "3. The `tpe.suggest` optimization algorithm\n",
    "4. A `SparkTrials` object to distribute the trials across a cluster using Spark\n",
    "5. The maximum number of evaluations or trials denoted by `max_evals`\n",
    "\n",
    "In this case, we'll be computing up to 20 trials with 4 trials being run concurrently.\n",
    "\n",
    "When the optimization process is finished, we train a final model using those hyperparameter values on the entire training/cross-validation dataset.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> While Hyperopt automatically logs its trials to MLflow under a single parent run, we are going manually specifying that parent run to log our final trained model details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70d1fe2-8be0-49cb-a311-d521c9a7e39f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from hyperopt import fmin, tpe, STATUS_OK, SparkTrials\n",
    "import mlflow\n",
    "\n",
    "# Start a parent MLflow run\n",
    "mlflow.set_experiment(\"/Users/\" + username + \"/SMLP-Lesson-4\")\n",
    "with mlflow.start_run():\n",
    "    # The number of models we want to evaluate\n",
    "    num_evals = 20\n",
    "\n",
    "    # Set the number of models to be trained concurrently\n",
    "    spark_trials = SparkTrials(parallelism=4)\n",
    "\n",
    "    # Run the optimization process\n",
    "    best_hyperparam = fmin(\n",
    "        fn=objective_function, \n",
    "        space=search_space,\n",
    "        algo=tpe.suggest, \n",
    "        trials=spark_trials,\n",
    "        max_evals=num_evals\n",
    "    )\n",
    "\n",
    "    # Get optimal hyperparameter values\n",
    "    best_max_depth = int(best_hyperparam[\"max_depth\"])\n",
    "    best_n_estimators = int(best_hyperparam[\"n_estimators\"])\n",
    "\n",
    "    # Train model on entire training data\n",
    "    regressor = RandomForestRegressor(max_depth=best_max_depth, n_estimators=best_n_estimators, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluator on train and test set\n",
    "    train_rmse = mean_squared_error(y_train, regressor.predict(X_train), squared=False)\n",
    "    test_rmse = mean_squared_error(y_test, regressor.predict(X_test), squared=False)\n",
    "    \n",
    "    mlflow.log_param(\"max_depth\", best_max_depth)\n",
    "    mlflow.log_param(\"n_estimators\", best_n_estimators)\n",
    "    mlflow.log_metric(\"loss\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c19546-ccba-47a7-83ce-03cdeb544872",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2021 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SMLP 04 - Tuning Single-node Models with Hyperopt and Apache Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
